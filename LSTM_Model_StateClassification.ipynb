{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "0y0D2gd18noS",
        "outputId": "f9edbbca-1a04-4cc9-9951-eb174b7688c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-a6f4c3779092>:9: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
            "  telemetry_data =pd.read_csv(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>create_dt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-09-29 08:12:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-09-29 08:12:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-09-29 08:12:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-09-29 08:12:51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-09-29 08:12:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516068</th>\n",
              "      <td>2023-10-26 08:16:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516069</th>\n",
              "      <td>2023-10-26 08:16:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516070</th>\n",
              "      <td>2023-10-26 08:16:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516071</th>\n",
              "      <td>2023-10-26 08:16:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516072</th>\n",
              "      <td>2023-10-26 08:16:46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>516073 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> datetime64[ns]</label>"
            ],
            "text/plain": [
              "0        2023-09-29 08:12:46\n",
              "1        2023-09-29 08:12:47\n",
              "2        2023-09-29 08:12:49\n",
              "3        2023-09-29 08:12:51\n",
              "4        2023-09-29 08:12:53\n",
              "                 ...        \n",
              "516068   2023-10-26 08:16:26\n",
              "516069   2023-10-26 08:16:31\n",
              "516070   2023-10-26 08:16:36\n",
              "516071   2023-10-26 08:16:41\n",
              "516072   2023-10-26 08:16:46\n",
              "Name: create_dt, Length: 516073, dtype: datetime64[ns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load Data\n",
        "telemetry_data =pd.read_csv(\n",
        "    'telemetry_for_operations_training_2.csv',\n",
        "    parse_dates=['create_dt'],\n",
        "    date_parser=lambda x: pd.to_datetime(x, errors='coerce')  # Handles coercion of invalid formats\n",
        ")\n",
        "labels_data = pd.read_csv(\"operations_labels_training.csv\")\n",
        "telemetry_data['create_dt']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7DhvUDOJz1J",
        "outputId": "656fcddf-1840-426d-8e11-90662cfb17a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-230bd17d4208>:8: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
            "  validation_data = pd.read_csv(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load the trained model\n",
        "rf_classifier = joblib.load('random_forest_model.pkl')\n",
        "\n",
        "# Step 2: Load the Validation Data\n",
        "validation_data = pd.read_csv(\n",
        "    'telemetry_for_operations_validation.csv',\n",
        "    parse_dates=['create_dt'],\n",
        "    date_parser=lambda x: pd.to_datetime(x, errors='coerce')  # Handles coercion of invalid formats\n",
        ")\n",
        "# Step 3: Preprocess DateTime Columns\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Eih87ympKC-4",
        "outputId": "eeb73038-8868-49fb-b704-c05d54c3c041"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>create_dt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-09-29 08:03:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-09-29 08:03:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-09-29 08:03:33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-09-29 08:03:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-09-29 08:03:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260106</th>\n",
              "      <td>2023-09-30 16:32:25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260107</th>\n",
              "      <td>2023-09-30 16:32:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260108</th>\n",
              "      <td>2023-09-30 16:32:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260109</th>\n",
              "      <td>2023-09-30 16:32:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260110</th>\n",
              "      <td>2023-09-30 16:32:45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>260111 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> datetime64[ns]</label>"
            ],
            "text/plain": [
              "0        2023-09-29 08:03:27\n",
              "1        2023-09-29 08:03:28\n",
              "2        2023-09-29 08:03:33\n",
              "3        2023-09-29 08:03:34\n",
              "4        2023-09-29 08:03:35\n",
              "                 ...        \n",
              "260106   2023-09-30 16:32:25\n",
              "260107   2023-09-30 16:32:30\n",
              "260108   2023-09-30 16:32:35\n",
              "260109   2023-09-30 16:32:40\n",
              "260110   2023-09-30 16:32:45\n",
              "Name: create_dt, Length: 260111, dtype: datetime64[ns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_data['create_dt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tZK1s0v9tun",
        "outputId": "bac90d4b-53b2-4404-d99b-bcca5acc7fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows with missing create_dt values:\n",
            "       create_dt  mdm_object_name  mdm_model_id       lon       lat  alt  \\\n",
            "60902        NaT             1492            62  0.017198  0.006647   39   \n",
            "102168       NaT             1425            62  0.014344  0.002874  -37   \n",
            "184505       NaT             1035            62 -0.040465  0.011229  194   \n",
            "201253       NaT              125            61  0.024091 -0.021479  123   \n",
            "\n",
            "        speed_gps  direction  accel_forward_nn  accel_braking_nn  \\\n",
            "60902         0.0       42.0             0.000             0.000   \n",
            "102168       19.9        0.0             0.686             0.000   \n",
            "184505       23.2      170.0             0.000             0.294   \n",
            "201253       17.1       69.0             0.980             0.000   \n",
            "\n",
            "        accel_angular_nn  accel_vertical_nn  \n",
            "60902                0.0                0.0  \n",
            "102168               0.0                0.0  \n",
            "184505               0.0                0.0  \n",
            "201253               0.0                0.0  \n",
            "Predictions saved to 'predicted_validation_data.csv'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 4: Check for any missing values after conversion\n",
        "missing_dates = validation_data[validation_data['create_dt'].isna()]\n",
        "\n",
        "# Print the rows with missing create_dt values\n",
        "print(\"Rows with missing create_dt values:\")\n",
        "print(missing_dates)\n",
        "\n",
        "# Step 5: Feature Engineering - Extract useful features from datetime columns\n",
        "validation_data['hour'] = validation_data['create_dt'].dt.hour\n",
        "validation_data['day_of_week'] = validation_data['create_dt'].dt.weekday\n",
        "validation_data['month'] = validation_data['create_dt'].dt.month\n",
        "validation_data['second'] = validation_data['create_dt'].dt.second  # Extract seconds for later use if needed\n",
        "\n",
        "# Step 6: Define Features for Prediction\n",
        "# Features: Choose telemetry metrics and the extracted datetime features\n",
        "features_validation = validation_data[['speed_gps', 'accel_forward_nn', 'accel_braking_nn',\n",
        "                                       'accel_angular_nn', 'accel_vertical_nn',\n",
        "                                       'hour', 'day_of_week', 'month']]\n",
        "\n",
        "# Step 7: Make Predictions\n",
        "validation_data['predicted_operation_state'] = rf_classifier.predict(features_validation)\n",
        "\n",
        "# Step 8: Map Predicted Values to Integer Codes as Specified\n",
        "state_mapping = {'idle': 0, 'loading': 1, 'riding_loaded': 2, 'unloading_lift': 3, 'riding_empty': 5}\n",
        "validation_data['predicted_operation_state'] = validation_data['predicted_operation_state'].map(state_mapping)\n",
        "\n",
        "# Step 9: Retain the original create_dt with seconds for the output\n",
        "validation_data['create_dt'] = validation_data['create_dt'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Step 10: Save Predictions to a New CSV File\n",
        "# Include only the columns 'create_dt', 'mdm_object_name', and 'predicted_operation_state'\n",
        "output_columns = ['create_dt', 'mdm_object_name', 'predicted_operation_state']\n",
        "validation_data[output_columns].to_csv('predicted_validation_data.csv', index=False, lineterminator='\\r\\n')\n",
        "\n",
        "print(\"Predictions saved to 'predicted_validation_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XTiT-rDKMyX",
        "outputId": "f4798634-0163-4428-a3eb-557ad4307ffe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-d4758a061414>:9: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
            "  telemetry_data = pd.read_csv(\n",
            "<ipython-input-11-d4758a061414>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  telemetry_data.loc[:, 'hour'] = telemetry_data['create_dt'].dt.hour\n",
            "<ipython-input-11-d4758a061414>:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  telemetry_data.loc[:, 'day_of_week'] = telemetry_data['create_dt'].dt.weekday\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 10ms/step - accuracy: 0.6900 - loss: 0.7990 - val_accuracy: 0.7698 - val_loss: 0.5821\n",
            "Epoch 2/3\n",
            "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.7716 - loss: 0.5773 - val_accuracy: 0.7864 - val_loss: 0.5352\n",
            "Epoch 3/3\n",
            "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.7926 - loss: 0.5274 - val_accuracy: 0.8085 - val_loss: 0.4809\n",
            "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.8079 - loss: 0.4819\n",
            "Model Evaluation: [0.4809390604496002, 0.8085324168205261]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Step 1: Load the Datasets\n",
        "telemetry_data = pd.read_csv(\n",
        "    'telemetry_for_operations_training_2.csv',\n",
        "    parse_dates=['create_dt'],\n",
        "    date_parser=lambda x: pd.to_datetime(x, errors='coerce')\n",
        ")\n",
        "\n",
        "labels_data = pd.read_csv(\"operations_labels_training.csv\")\n",
        "\n",
        "# Step 2: Preprocess DateTime Columns\n",
        "telemetry_data['create_dt'] = pd.to_datetime(telemetry_data['create_dt'], errors='coerce')\n",
        "labels_data['start_time'] = pd.to_datetime(labels_data['start_time'], errors='coerce')\n",
        "labels_data['end_time'] = pd.to_datetime(labels_data['end_time'], errors='coerce')\n",
        "\n",
        "# Step 3: Create operation_state column based on time intervals\n",
        "telemetry_data['operation_kind_id'] = None\n",
        "\n",
        "for _, label_row in labels_data.iterrows():\n",
        "    mask = (\n",
        "        (telemetry_data['create_dt'] >= label_row['start_time']) &\n",
        "        (telemetry_data['create_dt'] <= label_row['end_time']) &\n",
        "        (telemetry_data['mdm_object_name'] == label_row['mdm_object_name'])\n",
        "    )\n",
        "    telemetry_data.loc[mask, 'operation_kind_id'] = label_row['operation_kind_id']\n",
        "\n",
        "# Drop rows with missing operation states\n",
        "telemetry_data = telemetry_data.dropna(subset=['operation_kind_id'])\n",
        "\n",
        "# Step 4: Feature Engineering with .loc to avoid SettingWithCopyWarning\n",
        "telemetry_data.loc[:, 'hour'] = telemetry_data['create_dt'].dt.hour\n",
        "telemetry_data.loc[:, 'day_of_week'] = telemetry_data['create_dt'].dt.weekday\n",
        "telemetry_data.loc[:, 'month'] = telemetry_data['create_dt'].dt.month\n",
        "\n",
        "# Selected features for the LSTM model\n",
        "features = telemetry_data[['speed_gps', 'accel_forward_nn', 'accel_braking_nn',\n",
        "                           'accel_angular_nn', 'accel_vertical_nn', 'hour',\n",
        "                           'day_of_week', 'month']]\n",
        "\n",
        "# Step 5: Normalize Feature Values\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Step 6: Create Sequences of Data\n",
        "sequence_length = 30  # Number of time steps to consider in each sequence\n",
        "\n",
        "def create_sequences(features, labels, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - seq_length):\n",
        "        X.append(features[i:i + seq_length])\n",
        "        y.append(labels[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Use operation_kind_id directly as labels\n",
        "labels = telemetry_data['operation_kind_id'].values\n",
        "\n",
        "X, y = create_sequences(scaled_features, labels, sequence_length)\n",
        "\n",
        "# Step 7: Split Data into Training and Testing Sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Build the LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(6, activation='softmax'))  # 6 because classes are 0, 1, 2, 3, 5\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 9: Train the Model\n",
        "checkpoint = ModelCheckpoint('lstm_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3,\n",
        "                    batch_size=128, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Step 10: Save the Model\n",
        "model.save('/content/drive/MyDrive/OES/lstm_model.keras')\n",
        "print(f\"Model Evaluation: {model.evaluate(X_test, y_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "q8F59F0PXeeq",
        "outputId": "74953f45-9b00-4886-d24d-f3b0d85b5c0f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/OES/operations_label_training.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9bfcdd72b146>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Step 2: Load Training Data and Fit Scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load your training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/OES/operations_label_training.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Assuming your training features are the same as your validation features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/OES/operations_label_training.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Step 1: Load the pre-trained model\n",
        "model = load_model('lstm_model.keras')\n",
        "\n",
        "# Step 2: Load Training Data and Fit Scaler\n",
        "# Load your training data\n",
        "training_data = pd.read_csv('operations_label_training.csv')\n",
        "\n",
        "# Assuming your training features are the same as your validation features\n",
        "# Adjust this part based on your actual training features\n",
        "training_features = training_data[['speed_gps', 'accel_forward_nn', 'accel_braking_nn',\n",
        "                                   'accel_angular_nn', 'accel_vertical_nn', 'hour',\n",
        "                                   'day_of_week', 'month']]\n",
        "\n",
        "# Fit the scaler on training features\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(training_features)\n",
        "\n",
        "# Step 3: Load and Preprocess Validation Data\n",
        "validation_data = pd.read_csv(\n",
        "    'telemetry_for_operations_validation.csv',\n",
        "    parse_dates=['create_dt'],\n",
        "    date_parser=lambda x: pd.to_datetime(x, errors='coerce')\n",
        ")\n",
        "\n",
        "# Preprocess validation data similar to training data\n",
        "validation_data['create_dt'] = pd.to_datetime(validation_data['create_dt'], errors='coerce')\n",
        "validation_data['hour'] = validation_data['create_dt'].dt.hour\n",
        "validation_data['day_of_week'] = validation_data['create_dt'].dt.weekday\n",
        "validation_data['month'] = validation_data['create_dt'].dt.month\n",
        "\n",
        "# Step 4: Extract and Normalize Validation Features\n",
        "validation_features = validation_data[['speed_gps', 'accel_forward_nn', 'accel_braking_nn',\n",
        "                                       'accel_angular_nn', 'accel_vertical_nn', 'hour',\n",
        "                                       'day_of_week', 'month']]\n",
        "\n",
        "# Normalize validation features using the fitted scaler from training data\n",
        "validation_features = scaler.transform(validation_features)\n",
        "\n",
        "# Step 5: Create Sequences for Validation Data\n",
        "sequence_length = 30  # This should match the sequence length used during training\n",
        "\n",
        "def create_sequences(features, seq_length):\n",
        "    X = []\n",
        "    for i in range(len(features) - seq_length):\n",
        "        X.append(features[i:i + seq_length])\n",
        "    return np.array(X)\n",
        "\n",
        "X_val = create_sequences(validation_features, sequence_length)\n",
        "\n",
        "# Step 6: Make Predictions on Validation Data\n",
        "validation_predictions = model.predict(X_val)\n",
        "\n",
        "# Convert predictions to label indices using `np.argmax`\n",
        "predicted_label_indices = np.argmax(validation_predictions, axis=1)\n",
        "\n",
        "# Ensure that the predicted indices are integers\n",
        "predicted_label_indices = predicted_label_indices.astype(int)\n",
        "\n",
        "# Step 7: Insert the predicted label indices into the validation dataframe\n",
        "validation_data['predicted_operation_state'] = np.nan\n",
        "\n",
        "# Align the predicted labels with the original data considering sequence_length offset\n",
        "predicted_indices = range(sequence_length, sequence_length + len(predicted_label_indices))\n",
        "validation_data.loc[predicted_indices, 'predicted_operation_state'] = predicted_label_indices\n",
        "\n",
        "# Step 8: Fill NaN values if required (optional)\n",
        "validation_data['predicted_operation_state'].fillna(-1, inplace=True)  # Or any placeholder\n",
        "\n",
        "# Step 9: Save Results to CSV\n",
        "validation_data[['create_dt', 'mdm_object_name', 'predicted_operation_state']].to_csv(\n",
        "    'predicted_validation_data_lstm_encoded.csv', index=False\n",
        ")\n",
        "\n",
        "# Optional: Display a sample of the mapped predictions to ensure alignment\n",
        "print(validation_data[['create_dt', 'mdm_object_name', 'predicted_operation_state']].head(40))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyQ5AFQaXrRI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
